{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align = \"center\" draggable=”false” ><img src=\"https://user-images.githubusercontent.com/37101144/161836199-fdb0219d-0361-4988-bf26-48b0fad160a3.png\"\n",
    "     width=\"200px\"\n",
    "     height=\"auto\"/>\n",
    "</p>\n",
    "\n",
    "# <h1 align=\"center\" id=\"heading\">FastAPI Web App with Hugging Face Transformers</h1>\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "### ☑️ Objectives\n",
    "At the end of this session, you will be able to:\n",
    "- [ ] Interact with the Hugging Face API\n",
    "- [ ] Build a simple web app using Fast API\n",
    "- [ ] Create an API endpoint for your selected pre-trained model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1: Hugging Face Model Hub\n",
    "\n",
    "First things first: We need to make a Hugging Face account! This is a free service that allows you to access pre-trained models. You can also upload your own models to the Hugging Face API!\n",
    "\n",
    "## 1.1 Create an account on Hugging Face\n",
    "\n",
    "1. Navigate to [Hugging Face](https://huggingface.co/) and click on the \"Sign Up\" button in the top right corner.\n",
    "\n",
    "2. Complete the sign-up process, and ensure you have verified your email address!\n",
    "\n",
    "## 1.2 Create a Hugging Face token\n",
    "\n",
    "Now that you have an account, you need to create a token. This token will allow you to access Hugging Face resources from your personal computer. While this is not always strictly necessary, it is a good idea to create a token for your own security.\n",
    "\n",
    "1. Navigate to your [Hugging Face profile](https://huggingface.co/settings/token) and click on the \"New token\" button.\n",
    "\n",
    "2. Once you have created a token, copy it and save it in the `.env` file in the root directory of this repository. \n",
    "\n",
    "3. Ensure that you have added the `.env` file to your `.gitignore` file. This will make sure that your token is not shared with the world!\n",
    "\n",
    "## 1.3 Browsing the Hugging Face Model Hub\n",
    "\n",
    "Now that you have an account and a token, you can start browsing the Hugging Face Model Hub. The Model Hub is a collection of pre-trained models that you can use for a variety of ML tasks.\n",
    "\n",
    "We're going to focus on translation this week, so let's start by browsing the NLP models. \n",
    "\n",
    "1. Navigate to the [NLP models](https://huggingface.co/models) page and click on the `+23 Tasks` (your number might be a bit different) button in the top left corner of the page. \n",
    "\n",
    "2. Select whatever task you want to explore. In this case, we're going to choose \"Translation\" and we'll select the [`t5-small`](https://huggingface.co/t5-small) model. \n",
    "\n",
    "3. Once you have selected a model, you can explore the model's documentation. This documentation will tell you how to use the model, and what it was trained on.\n",
    "\n",
    "4. You can also quickly import the model using the \"</> Use in Transformers\" button. This will give you a code snippet that you can use to import the model into your own code!\n",
    "\n",
    "Now that you have a model in mind, let's start working towards building a web app!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2: Hugging Face Pipelines\n",
    "\n",
    "Now that we've explored the Hugging Face Model Hub, let's take a look at the Hugging Face Pipelines. Pipelines are a simple way to use pre-trained models. They allow you to quickly and easily use pre-trained models without having to write any code!\n",
    "\n",
    "### 2.1 Using the Hugging Face Pipelines\n",
    "\n",
    "1. Navigate to the [Hugging Face Pipelines](https://huggingface.co/transformers/main_classes/pipelines.html) documentation page.\n",
    "\n",
    "2. Scroll down the TranslationPipeline to check out the documentation for the `pipeline` function we'll be using!"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m model \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39mT5Model\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mt5-small\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[39m# Defining tokenizer:\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m tokenizer \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39;49mT5Tokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mt5-small\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[39m#setting device to run on cpu until I can figure out FastAPI component...\u001b[39;00m\n\u001b[1;32m     13\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlops-course/lib/python3.10/site-packages/transformers/utils/import_utils.py:1009\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1007\u001b[0m \u001b[39mif\u001b[39;00m key\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   1008\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1009\u001b[0m requires_backends(\u001b[39mcls\u001b[39;49m, \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_backends)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlops-course/lib/python3.10/site-packages/transformers/utils/import_utils.py:997\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m    995\u001b[0m failed \u001b[39m=\u001b[39m [msg\u001b[39m.\u001b[39mformat(name) \u001b[39mfor\u001b[39;00m available, msg \u001b[39min\u001b[39;00m checks \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m available()]\n\u001b[1;32m    996\u001b[0m \u001b[39mif\u001b[39;00m failed:\n\u001b[0;32m--> 997\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "model = transformers.T5Model.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Defining tokenizer:\n",
    "tokenizer = transformers.T5Tokenizer.from_pretrained('t5-small')\n",
    " \n",
    "#setting device to run on cpu until I can figure out FastAPI component...\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "#create the pipeline\n",
    "pipeline = transformers.pipeline(\"translation_en_to_de\", model=model, tokenizer=tokenizer, device=device)\n",
    "print(translator(\"Where are we going?\", \"We are going to New York City!\", max_length=40))\n",
    "\n",
    "#input text \n",
    "input_text= \"Where are we going\"\n",
    "\n",
    "#set prefix for translation task:\n",
    "task_prefix = \"Translate English to German: \"\n",
    "\n",
    "#add it to input_text\n",
    "translate_text= prefix + input_text\n",
    "\n",
    "# use different length sentences to test batching\n",
    "\n",
    "output = pipeline(translate_text, max_length=1024)\n",
    "\n",
    "print(output[0]['translation_text'])\n",
    "\n",
    "\n",
    "# We're going to use the pipeline API to do inference on a pretrained model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m T5Tokenizer, T5Model\n\u001b[0;32m----> 3\u001b[0m tokenizer \u001b[39m=\u001b[39m T5Tokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mt5-small\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m model \u001b[39m=\u001b[39m T5Model\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mt5-small\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m input_ids \u001b[39m=\u001b[39m tokenizer(\n\u001b[1;32m      7\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mStudies have been shown that owning a dog is good for you\u001b[39m\u001b[39m\"\u001b[39m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m )\u001b[39m.\u001b[39minput_ids  \u001b[39m# Batch size 1\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlops-course/lib/python3.10/site-packages/transformers/utils/import_utils.py:1009\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1007\u001b[0m \u001b[39mif\u001b[39;00m key\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   1008\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1009\u001b[0m requires_backends(\u001b[39mcls\u001b[39;49m, \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_backends)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlops-course/lib/python3.10/site-packages/transformers/utils/import_utils.py:997\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m    995\u001b[0m failed \u001b[39m=\u001b[39m [msg\u001b[39m.\u001b[39mformat(name) \u001b[39mfor\u001b[39;00m available, msg \u001b[39min\u001b[39;00m checks \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m available()]\n\u001b[1;32m    996\u001b[0m \u001b[39mif\u001b[39;00m failed:\n\u001b[0;32m--> 997\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5Model\n",
    "\n",
    "tokenizer = transformers.T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5Model.from_pretrained(\"t5-small\")\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    \"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\"\n",
    ").input_ids  # Batch size 1\n",
    "decoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"pt\").input_ids  # Batch size 1\n",
    "\n",
    "# forward pass\n",
    "outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n",
    "last_hidden_states = outputs.last_hidden_state\n"
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "We're going to use the pipeline API to do inference on a pretrained model.\n",
    "\n",
    "We'll be making a translation pipeline, using the model you selected above!\n",
    "\"\"\"\n",
    "pipeline = None # YOUR CODE HERE"
>>>>>>> 246ef8a5e00ef3fdec5d8736d02d9e066943f171
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Save the pipeline locally\n",
    "\n",
    "Now that we know how to use the `pipeline` function, let's save it locally so we can use it in our web app later without needing to download it every time we run the app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the pipeline\n",
<<<<<<< HEAD
    "print(pipeline('T5_small_trial'))\n",
    "\n",
    "# Save the pipeline\n",
    "model_name =  # add a name for your model\n",
=======
    "print(pipeline('ENTER YOUR TEXT HERE'))\n",
    "\n",
    "# Save the pipeline\n",
    "model_name = None # add a name for your model\n",
>>>>>>> 246ef8a5e00ef3fdec5d8736d02d9e066943f171
    "pipeline.save_pretrained('fast_api_tutorial/app/model/' + model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONCLUSION\n",
    "\n",
    "Now that you have a Hugging Face account, a token, and saved the pipeline locally, you're ready to start building your web app!\n",
    "\n",
    "Let's get out of this notebook, and into the `fast_api_tutorial` subdirectory!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< HEAD
   "display_name": "mlops-course",
=======
   "display_name": "fourthbrain_fastapi",
>>>>>>> 246ef8a5e00ef3fdec5d8736d02d9e066943f171
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.10.8"
=======
   "version": "3.8.15 (default, Nov 24 2022, 15:19:38) \n[GCC 11.2.0]"
>>>>>>> 246ef8a5e00ef3fdec5d8736d02d9e066943f171
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
<<<<<<< HEAD
    "hash": "4957537c5eba6cefd9ad4e6033182e3f3f7760aa4d214ef3d2b6e025883c63e6"
=======
    "hash": "0630e3cd024c46826e25fa8bd0accd7839fef159a125d3091ceba697ba8b41dc"
>>>>>>> 246ef8a5e00ef3fdec5d8736d02d9e066943f171
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
