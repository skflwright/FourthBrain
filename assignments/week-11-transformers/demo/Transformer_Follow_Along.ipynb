{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-egkpUTSarLK"
      },
      "source": [
        "# In Class Follow Along Notebook\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJjtovQ9axry"
      },
      "source": [
        "First things first, we'll set-up the data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2i_bITLVY7zD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "OkIZ8Z6oZEKp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /home/skfl/anaconda3/envs/mle11/lib/python3.10/site-packages (2.11.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /home/skfl/anaconda3/envs/mle11/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /home/skfl/anaconda3/envs/mle11/lib/python3.10/site-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /home/skfl/anaconda3/envs/mle11/lib/python3.10/site-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /home/skfl/anaconda3/envs/mle11/lib/python3.10/site-packages (from tensorflow) (1.23.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /home/skfl/anaconda3/envs/mle11/lib/python3.10/site-packages (from tensorflow) (4.4.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /home/skfl/anaconda3/envs/mle11/lib/python3.10/site-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /home/skfl/anaconda3/envs/mle11/lib/python3.10/site-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/skfl/anaconda3/envs/mle11/lib/python3.10/site-packages (from tensorflow) (1.51.3)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /home/skfl/anaconda3/envs/mle11/lib/python3.10/site-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/skfl/anaconda3/envs/mle11/lib/python3.10/site-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: setuptools in /home/skfl/anaconda3/envs/mle11/lib/python3.10/site-packages (from tensorflow) (65.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /home/skfl/anaconda3/envs/mle11/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/skfl/anaconda3/envs/mle11/lib/python3.10/site-packages (from tensorflow) (0.31.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /home/skfl/anaconda3/envs/mle11/lib/python3.10/site-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /home/skfl/anaconda3/envs/mle11/lib/python3.10/site-packages (from tensorflow) (23.1.21)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /home/skfl/anaconda3/envs/mle11/lib/python3.10/site-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: packaging in /home/skfl/anaconda3/envs/mle11/lib/python3.10/site-packages (from tensorflow) (23.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /home/skfl/anaconda3/envs/mle11/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /home/skfl/anaconda3/envs/mle11/lib/python3.10/site-packages (from tensorflow) (15.0.6.1)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /home/skfl/anaconda3/envs/mle11/lib/python3.10/site-packages (from tensorflow) (2.11.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /home/skfl/anaconda3/envs/mle11/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/skfl/anaconda3/envs/mle11/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /home/skfl/anaconda3/envs/mle11/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /home/skfl/anaconda3/envs/mle11/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/skfl/anaconda3/envs/mle11/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /home/skfl/anaconda3/envs/mle11/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.28.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/skfl/anaconda3/envs/mle11/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/skfl/anaconda3/envs/mle11/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/skfl/anaconda3/envs/mle11/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /home/skfl/anaconda3/envs/mle11/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/skfl/anaconda3/envs/mle11/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/skfl/anaconda3/envs/mle11/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.2.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/skfl/anaconda3/envs/mle11/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/skfl/anaconda3/envs/mle11/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/skfl/anaconda3/envs/mle11/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/skfl/anaconda3/envs/mle11/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/skfl/anaconda3/envs/mle11/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.26.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/skfl/anaconda3/envs/mle11/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/skfl/anaconda3/envs/mle11/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /home/skfl/anaconda3/envs/mle11/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "!pip install tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers as L\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "NUM_LABELS = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "cleaned_tweets = pd.read_csv(\"cleaned_tweets.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "wP0EBj57ZZiw"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tidy_tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>when a father is dysfunctional and is so sel...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>thanks for #lyft credit i cant use cause the...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>bihday your majesty</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>#model   i love u take with u all the time in ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>factsguide society now    #motivation</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          tidy_tweet  label\n",
              "0    when a father is dysfunctional and is so sel...      0\n",
              "1    thanks for #lyft credit i cant use cause the...      0\n",
              "2                                bihday your majesty      0\n",
              "3  #model   i love u take with u all the time in ...      0\n",
              "4              factsguide society now    #motivation      0"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cleaned_tweets.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "YNdI8lPqZfBK"
      },
      "outputs": [],
      "source": [
        "X, y = pd.Series(cleaned_tweets['tidy_tweet']), pd.Series(cleaned_tweets['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "mckqeu1aZpfN"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train_sub, X_test, y_train_sub, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "VTWa4CbkaAeD"
      },
      "outputs": [],
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_sub, y_train_sub, test_size=0.2, stratify=y_train_sub, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "990gUduTa2a5"
      },
      "source": [
        "## Positional Embedding Layer\n",
        "\n",
        "We'll make the positional embedding layer as seen in the \"Attention is all you need\" paper!\n",
        "\n",
        "The idea behind Positional Encoding is fairly simple as well: to give the model access to token order information, therefore we are going to add the token's position in the sentence to each word embedding.\n",
        "\n",
        "Thus, one input word embedding will have two components: the usual token vector representing the token independent of any specific context, and a position vector representing the position of the token in the current sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "CPEA_KsZaky5"
      },
      "outputs": [],
      "source": [
        "### Positional Embedding\n",
        "from tensorflow.keras import layers as L\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "class PositionalEmbedding(L.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        self.token_embeddings = L.Embedding(input_dim, output_dim)\n",
        "        self.position_embeddings = L.Embedding(sequence_length, output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        super().__init__(**kwargs)\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "        \n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzCOplI_eJEf"
      },
      "source": [
        "## Transformer Block\n",
        "\n",
        "Recently most of the natural language processing tasks are being dominated by the Transformer architecture, introduced in [Attention Is All You Need](https://arxiv.org/abs/1706.03762), which used a simple mechanism called Neural Attention as one of its building blocks. As the title suggests this architecture didn't require any recurrent layer. We now build a text classification using Attention and Positional Embeddings.\n",
        "\n",
        "Transformer (attention) Block.\n",
        "\n",
        "The concept of Neural Attention is fairly simple; i.e., not all input information seen by a model is equally important to the task at hand. Although this concept has been utilized at various different places as well, e.g., max pooling in ConvNets, but the kind of attention we are looking for should be context aware.\n",
        "\n",
        "The attention mechanism allows output to focus attention on input while producing output while the self-attention model allows inputs to interact with each other; in other words, calculate attention of all other inputs with respect to one input.\n",
        "\n",
        "In the paper, the authors proposed another type of attention mechanism called multi-headed attention which refers to the fact that the outer space of the self attention layer gets factored into a set of independent sub-spaces learned separately, where each subspace is called a \"head\". You need to implement the multi-head attention layer, supplying values for two parameters: num_heads and key_dim.\n",
        "\n",
        "There is a learnable dense projection present after the multi-head attention which enables the layer to actually learn something, as opposed to being a purely stateless transformation. You need to implement dense_proj, use the tf.keras.Sequential to stack two dense layers:\n",
        "\n",
        " 1. first dense layer with `dense_dim` units and activation function `relu`;\n",
        " 2. second dense layer with `embed_dim` units and no activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "mSQCJSvTecQ0"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(L.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = L.MultiheadAttenion(num_heads, embed_dim)    \n",
        "        self.dense_proj = keras.Sequential([\n",
        "            L.Dense(dense_dim, activation='relu'),\n",
        "            L.Dense(embed_dim)\n",
        "            ])\n",
        "        self.layernorm1 = L.LayerNormalization()\n",
        "        self.layernorm2 = L.LayerNormalization()\n",
        "        super().__init__(**kwargs)\n",
        "    \n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[: tf.newaxis, :]\n",
        "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm2(proj_input + proj_output)\n",
        "    \n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGlaqhELc4XJ"
      },
      "source": [
        "## Transformer Model in Keras\n",
        "\n",
        "Let's build it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "kGU1mvM5dBCR"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = 10_000\n",
        "EMBED_DIM = 256\n",
        "DENSE_DIM = 32\n",
        "NUM_HEADS = 2\n",
        "MAX_LEN = 256"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FQNdZcOdEwf"
      },
      "source": [
        "Tokenizer.\n",
        "\n",
        "The tokenizer is a simple tool to convert a text into a sequence of tokens. It is used to convert the training data into a sequence of integers, which are then used as input to the model.\n",
        "\n",
        "Use Tokenizer to create a tokenizer for the training data. Set the num_words parameter to the number of words to keep in the vocabulary, and oov_token to be \"\\<unk>\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "FpvW57zwdCrW"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'int' object has no attribute 'lower'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m/home/skfl/work/assignments/week-11-transformers/demo/Transformer_Follow_Along.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/skfl/work/assignments/week-11-transformers/demo/Transformer_Follow_Along.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m Tokenizer\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/skfl/work/assignments/week-11-transformers/demo/Transformer_Follow_Along.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m Tokenizer(num_words \u001b[39m=\u001b[39m VOCAB_SIZE, oov_token\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m<unk>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/skfl/work/assignments/week-11-transformers/demo/Transformer_Follow_Along.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m tokenizer\u001b[39m.\u001b[39;49mfit_on_texts(X_train)\n",
            "File \u001b[0;32m~/anaconda3/envs/mle11/lib/python3.10/site-packages/keras/preprocessing/text.py:287\u001b[0m, in \u001b[0;36mTokenizer.fit_on_texts\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlower:\n\u001b[1;32m    286\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(text, \u001b[39mlist\u001b[39m):\n\u001b[0;32m--> 287\u001b[0m         text \u001b[39m=\u001b[39m [text_elem\u001b[39m.\u001b[39mlower() \u001b[39mfor\u001b[39;00m text_elem \u001b[39min\u001b[39;00m text]\n\u001b[1;32m    288\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    289\u001b[0m         text \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39mlower()\n",
            "File \u001b[0;32m~/anaconda3/envs/mle11/lib/python3.10/site-packages/keras/preprocessing/text.py:287\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlower:\n\u001b[1;32m    286\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(text, \u001b[39mlist\u001b[39m):\n\u001b[0;32m--> 287\u001b[0m         text \u001b[39m=\u001b[39m [text_elem\u001b[39m.\u001b[39;49mlower() \u001b[39mfor\u001b[39;00m text_elem \u001b[39min\u001b[39;00m text]\n\u001b[1;32m    288\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    289\u001b[0m         text \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39mlower()\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'lower'"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(num_words = VOCAB_SIZE, oov_token= \"<unk>\")\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qJJ073cdMuF"
      },
      "source": [
        "Pad the sequences.\n",
        "\n",
        "The tokenizer outputs a sequence of integers, which are then used as input to the model. However, the model expects a sequence of fixed length. To pad the sequences to the same length, use sequence.pad_sequences from keras.preprocessing.\n",
        "\n",
        "Complete function preprocess below to 1) tokenize the texts 2) pad the sequences to the same length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "ABEZtL1fdNVP"
      },
      "outputs": [],
      "source": [
        "from keras.utils import pad_sequences\n",
        "\n",
        "def preprocess(texts, tokenizer, maxlen:int = MAX_LEN):\n",
        "  tokenized_text = tokenizer.texts_to_sequences(texts)\n",
        "  seqs = pad_sequences(tokenized_text, maxlen=maxlen)\n",
        "  return tokenized_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFMp4gygdQXr"
      },
      "source": [
        "Preprocess the data.\n",
        "\n",
        "Use preprocess to preprocess the training, validation, and test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "M_64X4SUdRWt"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'int' object has no attribute 'lower'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m/home/skfl/work/assignments/week-11-transformers/demo/Transformer_Follow_Along.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/skfl/work/assignments/week-11-transformers/demo/Transformer_Follow_Along.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m X_train \u001b[39m=\u001b[39m  preprocess(X_train, tokenizer)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/skfl/work/assignments/week-11-transformers/demo/Transformer_Follow_Along.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m X_valid \u001b[39m=\u001b[39m  preprocess(X_valid, tokenizer)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/skfl/work/assignments/week-11-transformers/demo/Transformer_Follow_Along.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m X_test  \u001b[39m=\u001b[39m  preprocess(X_test, tokenizer)\n",
            "\u001b[1;32m/home/skfl/work/assignments/week-11-transformers/demo/Transformer_Follow_Along.ipynb Cell 22\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(texts, tokenizer, maxlen)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/skfl/work/assignments/week-11-transformers/demo/Transformer_Follow_Along.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess\u001b[39m(texts, tokenizer, maxlen:\u001b[39mint\u001b[39m \u001b[39m=\u001b[39m MAX_LEN):\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/skfl/work/assignments/week-11-transformers/demo/Transformer_Follow_Along.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m   tokenized_text \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mtexts_to_sequences(texts)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/skfl/work/assignments/week-11-transformers/demo/Transformer_Follow_Along.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m   seqs \u001b[39m=\u001b[39m pad_sequences(tokenized_text, maxlen\u001b[39m=\u001b[39mmaxlen)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/skfl/work/assignments/week-11-transformers/demo/Transformer_Follow_Along.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m tokenized_text\n",
            "File \u001b[0;32m~/anaconda3/envs/mle11/lib/python3.10/site-packages/keras/preprocessing/text.py:357\u001b[0m, in \u001b[0;36mTokenizer.texts_to_sequences\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtexts_to_sequences\u001b[39m(\u001b[39mself\u001b[39m, texts):\n\u001b[1;32m    346\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Transforms each text in texts to a sequence of integers.\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \n\u001b[1;32m    348\u001b[0m \u001b[39m    Only top `num_words-1` most frequent words will be taken into account.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[39m        A list of sequences.\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 357\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtexts_to_sequences_generator(texts))\n",
            "File \u001b[0;32m~/anaconda3/envs/mle11/lib/python3.10/site-packages/keras/preprocessing/text.py:380\u001b[0m, in \u001b[0;36mTokenizer.texts_to_sequences_generator\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlower:\n\u001b[1;32m    379\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(text, \u001b[39mlist\u001b[39m):\n\u001b[0;32m--> 380\u001b[0m         text \u001b[39m=\u001b[39m [text_elem\u001b[39m.\u001b[39mlower() \u001b[39mfor\u001b[39;00m text_elem \u001b[39min\u001b[39;00m text]\n\u001b[1;32m    381\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    382\u001b[0m         text \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39mlower()\n",
            "File \u001b[0;32m~/anaconda3/envs/mle11/lib/python3.10/site-packages/keras/preprocessing/text.py:380\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlower:\n\u001b[1;32m    379\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(text, \u001b[39mlist\u001b[39m):\n\u001b[0;32m--> 380\u001b[0m         text \u001b[39m=\u001b[39m [text_elem\u001b[39m.\u001b[39;49mlower() \u001b[39mfor\u001b[39;00m text_elem \u001b[39min\u001b[39;00m text]\n\u001b[1;32m    381\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    382\u001b[0m         text \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39mlower()\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'lower'"
          ]
        }
      ],
      "source": [
        "X_train =  preprocess(X_train, tokenizer)\n",
        "X_valid =  preprocess(X_valid, tokenizer)\n",
        "X_test  =  preprocess(X_test, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEw_iUdLdVod"
      },
      "source": [
        "Define the model with the following architecture:\n",
        "\n",
        "* Input Layer\n",
        "* Positional Embeddings\n",
        "* Transformer Block\n",
        "* Pooling\n",
        "* Dropout\n",
        "* Output Layer\n",
        "\n",
        "If you are not familiar with keras functional API, take a read [here](https://keras.io/guides/functional_api/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "nj6VLLiRdW3u"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-02-25 16:37:36.269150: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
            "2023-02-25 16:37:36.269222: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2023-02-25 16:37:36.269306: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-N8BFUMI): /proc/driver/nvidia/version does not exist\n",
            "2023-02-25 16:37:36.269611: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "module 'keras.api._v2.keras.layers' has no attribute 'MultiheadAttenion'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m/home/skfl/work/assignments/week-11-transformers/demo/Transformer_Follow_Along.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/skfl/work/assignments/week-11-transformers/demo/Transformer_Follow_Along.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m inputs \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39m(\u001b[39mNone\u001b[39;00m, ), dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mint64\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/skfl/work/assignments/week-11-transformers/demo/Transformer_Follow_Along.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m x \u001b[39m=\u001b[39m PositionalEmbedding(MAX_LEN, VOCAB_SIZE, EMBED_DIM)(inputs) \u001b[39m# YOUR CODE HERE\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/skfl/work/assignments/week-11-transformers/demo/Transformer_Follow_Along.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m x \u001b[39m=\u001b[39m TransformerBlock(EMBED_DIM, DENSE_DIM, NUM_HEADS)(x) \u001b[39m# YOUR CODE HERE\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/skfl/work/assignments/week-11-transformers/demo/Transformer_Follow_Along.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m x \u001b[39m=\u001b[39m L\u001b[39m.\u001b[39mGlobalMaxPooling1D()(x)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/skfl/work/assignments/week-11-transformers/demo/Transformer_Follow_Along.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m x \u001b[39m=\u001b[39m L\u001b[39m.\u001b[39mDropout(\u001b[39m0.1\u001b[39m)(x)\n",
            "\u001b[1;32m/home/skfl/work/assignments/week-11-transformers/demo/Transformer_Follow_Along.ipynb Cell 24\u001b[0m in \u001b[0;36mTransformerBlock.__init__\u001b[0;34m(self, embed_dim, dense_dim, num_heads, **kwargs)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/skfl/work/assignments/week-11-transformers/demo/Transformer_Follow_Along.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense_dim \u001b[39m=\u001b[39m dense_dim\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/skfl/work/assignments/week-11-transformers/demo/Transformer_Follow_Along.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads \u001b[39m=\u001b[39m num_heads\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/skfl/work/assignments/week-11-transformers/demo/Transformer_Follow_Along.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention \u001b[39m=\u001b[39m L\u001b[39m.\u001b[39;49mMultiheadAttenion(num_heads, embed_dim)    \n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/skfl/work/assignments/week-11-transformers/demo/Transformer_Follow_Along.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense_proj \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mSequential([\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/skfl/work/assignments/week-11-transformers/demo/Transformer_Follow_Along.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     L\u001b[39m.\u001b[39mDense(dense_dim, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/skfl/work/assignments/week-11-transformers/demo/Transformer_Follow_Along.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     L\u001b[39m.\u001b[39mDense(embed_dim)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/skfl/work/assignments/week-11-transformers/demo/Transformer_Follow_Along.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     ])\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/skfl/work/assignments/week-11-transformers/demo/Transformer_Follow_Along.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayernorm1 \u001b[39m=\u001b[39m L\u001b[39m.\u001b[39mLayerNormalization()\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'keras.api._v2.keras.layers' has no attribute 'MultiheadAttenion'"
          ]
        }
      ],
      "source": [
        "inputs = keras.Input(shape=(None, ), dtype=\"int64\")\n",
        "x = PositionalEmbedding(MAX_LEN, VOCAB_SIZE, EMBED_DIM)(inputs) # YOUR CODE HERE\n",
        "x = TransformerBlock(EMBED_DIM, DENSE_DIM, NUM_HEADS)(x) # YOUR CODE HERE\n",
        "x = L.GlobalMaxPooling1D()(x)\n",
        "x = L.Dropout(0.1)(x)\n",
        "outputs = L.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vBkk6dAdox1"
      },
      "source": [
        "Compile model.\n",
        "\n",
        "Use 'adam' for the optimizer and accuracy for metrics, supply the correct value for loss.\n",
        "\n",
        "Remember, this is a binary classification task!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0H-JOY7dpa8"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer='adam', # YOUR CODE HERE\n",
        "    loss='binary_crossentropy', # YOUR CODE HERE\n",
        "    metrics=['accuracy']) # YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_noMiW2dss4"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mf4WnvTdu2s"
      },
      "source": [
        "Add [EarlyStopping](https://keras.io/api/callbacks/early_stopping/) and [ReduceLROnPlateau](https://keras.io/api/callbacks/reduce_lr_on_plateau/) to stop training if the model does not improve a set metric after a given number of epochs.\n",
        "\n",
        "Create an EarlyStopping object named es to stop training if the validation loss does not improve after 5 epochs. Set verbose to display messages when the callback takes an action and set restore_best_weights to restore model weights from the epoch with the best value of the monitored metric.\n",
        "\n",
        "Use ReduceLROnPlateau to reduce the learning rate if the validation loss does not improve after 3 epochs. Set verbose to display messages when the callback takes an action and use default values for other parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A221divwd2pX"
      },
      "outputs": [],
      "source": [
        "es =  # YOUR CODE HERE\n",
        "rlp =  # YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prI5VSd5d5ju"
      },
      "source": [
        "Train the model.\n",
        "\n",
        "Supply both EarlyStopping and ReduceLROnPlateau for callbacks. Set epochs to 100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxTZtZ_nd8Cf"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    X_train, y_train, \n",
        "    validation_data=(X_valid, y_valid),\n",
        "    # YOUR CODE HERE\n",
        "    epochs=10\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCs1ljq5d-fl"
      },
      "source": [
        "Evaluate the trained model on the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yk20ucAWd_hZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vpr4AktheCHj"
      },
      "source": [
        "Visualize both loss and accuracy curves for the training and validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UQh-ADKeDUD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "mle11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "f5d6e23cc21c3c22d5d038dfc2dc514a112c246cb6af6beae11f4484b62ad85b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
